{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f910f0f1-ba64-40ab-9b61-40c4cfba7e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, sampler\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from transformers import get_scheduler\n",
    "from datasets import Dataset\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584f5e19-7a54-4b6f-b7c8-2dcdbfd5958c",
   "metadata": {},
   "source": [
    "### Build HuggingFace Dataset from folder\n",
    "\n",
    "I should build an HF `Dataset` object and then use with a PyTorch `DataLoader` as in: https://huggingface.co/docs/datasets/en/use_with_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9082e715-4710-4529-929d-0b032b76ea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "label_map = {'happy': 1, 'sad': 0}\n",
    "\n",
    "# Download dataset from: https://github.com/mohummedalee/twitteraae-sentiment-data/\n",
    "def load_twitter_aae(dir):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    dialects = []\n",
    "    for dial in ['aae', 'sae']:\n",
    "        for lab in ['happy', 'sad']:\n",
    "            # load dialect x sentiment combination\n",
    "            fpath = os.path.join(dir, f'{dial}_{lab}')\n",
    "            with open(fpath, 'r', encoding='utf-8') as fh:\n",
    "                try:\n",
    "                    for line in fh:\n",
    "                        sentences.append(line.strip())\n",
    "                        labels.append(label_map[lab])\n",
    "                        dialects.append(dial.upper())\n",
    "                except UnicodeDecodeError:\n",
    "                    pass\n",
    "\n",
    "    return sentences, labels, dialects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6285b15e-525c-49f2-bb3f-576885d89202",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data/raw/sentiment_race'\n",
    "sentences, labels, dialects = load_twitter_aae(DATA_DIR)\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    'text': sentences,\n",
    "    'label': labels,\n",
    "    'dialect': dialects\n",
    "}).with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f532be4a-fb4e-4d79-a910-7228f4179837",
   "metadata": {},
   "source": [
    "### Set up model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4bfde5d-737f-4ce1-9f75-8402a3f1cb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"FacebookAI/roberta-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "63d0f3b0-b872-49dd-82c4-e7d4581c5773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=3):   0%|          | 0/12473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/muh.ali/miniconda3/envs/fairness-privacy/lib/python3.11/site-packages/datasets/table.py:1381: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/Users/muh.ali/miniconda3/envs/fairness-privacy/lib/python3.11/site-packages/datasets/table.py:1407: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "# run tokenizer on dataset using datasets .map function\n",
    "MAXLEN = 128\n",
    "\n",
    "def tokenize(instance):\n",
    "    tokenized = tokenizer(instance['text'], truncation=True, padding=\"max_length\", max_length=MAXLEN)\n",
    "    # return {**tokenized, \"label\": instance['label'], \"dialect\": instance['dialect']}\n",
    "    return {**tokenized}\n",
    "    \n",
    "dataset = dataset.map(tokenize, num_proc=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0abcf93-a24a-4384-94e7-38c4066b78ec",
   "metadata": {},
   "source": [
    "**TODOs**\n",
    "- split into train and validation set, build pytorch dataloaders for both\n",
    "- run raw training loop on the train_dataloader (as in https://huggingface.co/docs/transformers/en/training#train-in-native-pytorch) --- note that we don't want to use the `Trainer` API and should learn to write our own training loops since we will need to do that anyway when using `private-transformers` later (https://github.com/lxuechen/private-transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd38ffb1-19b3-4489-8a1e-ec531748825d",
   "metadata": {},
   "source": [
    "### Split into train, test, validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2da346d8-f1c8-42ac-a8b7-e5329fdd2049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialect counts: Counter({'SAE': 6723, 'AAE': 5750})\n",
      "train_inds: [6611 3681 8943 ... 6466 7430 3730] val_inds: [4651  901  304 ... 9637 2079 5768] test_inds: [4482 8169 4782 ... 7879  568 1106]\n"
     ]
    }
   ],
   "source": [
    "# inspect if there's gross imbalances in dialect -- doesn't seem so\n",
    "print('Dialect counts:', Counter(dataset['dialect']))\n",
    "\n",
    "# SubsetRandomSampler example: https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets/50544887#50544887\n",
    "\n",
    "shuffle_dataset = True\n",
    "random_seed = 419  # april 19\n",
    "N = len(dataset)\n",
    "inds = np.arange(N)\n",
    "train_size, val_size = 0.8, 0.1\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(inds)\n",
    "# pull out indices to pass to DataLoader\n",
    "train_split = int(N*train_size)\n",
    "train_inds = inds[:train_split]\n",
    "val_split = int(N*val_size)\n",
    "val_inds = inds[train_split:train_split+val_split]\n",
    "test_inds = inds[train_split+val_split:]\n",
    "\n",
    "print('train_inds:', train_inds, 'val_inds:', val_inds, 'test_inds:', test_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "22c63ebf-af67-4c8f-a986-eb1febb29772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn into pytorch-appropriate names\n",
    "# dataset = dataset.remove_columns(['text'])\n",
    "dataset = dataset.rename_column('label', 'labels')\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_sampler = sampler.SubsetRandomSampler(train_inds)\n",
    "val_sampler = sampler.SubsetRandomSampler(val_inds)\n",
    "test_sampler = sampler.SubsetRandomSampler(test_inds)\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, sampler=train_sampler)\n",
    "val_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, sampler=val_sampler)\n",
    "test_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75377b70-74c1-4d3f-9d3d-085c2ea7b06f",
   "metadata": {},
   "source": [
    "### Fine-tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c492c091-3fa8-499a-8a0d-e453d4106e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "# check if acceleration available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8777c73f-c548-4105-94c2-4b6b5342d552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9aec6785ea4523b12b16bc98e4b804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3744 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "# NOTE: not using lr_scheduler for now\n",
    "\n",
    "model.train()  # set to training mode\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        # includes input_ids, attention_mask, labels etc.\n",
    "        batch_topass = {\n",
    "            'input_ids': batch['input_ids'].to(device),\n",
    "            'attention_mask': batch['attention_mask'].to(device),\n",
    "            'labels': batch['labels'].to(device)\n",
    "        }  # things like dialect and text are not passed to the model\n",
    "        outputs = model(**batch_topass)  # unpack dict and pass as kwargs\n",
    "        \n",
    "        # normally, i'd have to compute the loss with a custom loss_fn\n",
    "        # but in HF, it's part of model output for convenience\n",
    "        loss = outputs.loss\n",
    "        loss.backward()  # compute gradients (based on `labels` passed to model)\n",
    "\n",
    "        optimizer.step()  # gradient update based on current training rate\n",
    "        optimizer.zero_grad()  # clear out gradients, compute new ones for next batch\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2b8c23a1-d265-41f4-be4f-8577fbdfea23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('models/roberta-no-privacy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835b1008-cf6b-4d8e-bf2e-cbd3cbb1eaa9",
   "metadata": {},
   "source": [
    "### Evaluate model on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "89c6ca61-fd80-4b92-abbc-d9465db16e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "# if not installed, run:\n",
    "# !conda install -y evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e7cc1899-e23c-43a9-b23e-c772694db426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f49cd330913445db8d66d2a40a928a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/156 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7417802726543705}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = evaluate.load('accuracy')\n",
    "\n",
    "model.eval()  # switch to eval mode\n",
    "for batch in tqdm(val_dataloader):\n",
    "    batch_topass = {\n",
    "        'input_ids': batch['input_ids'].to(device),\n",
    "        'attention_mask': batch['attention_mask'].to(device),\n",
    "        'labels': batch['labels'].to(device)\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch_topass)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "    metric.add_batch(predictions=predictions, references=batch['labels'])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42043f1b-d19c-4681-bbb9-54f24bba9347",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
