{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f910f0f1-ba64-40ab-9b61-40c4cfba7e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from datasets import Dataset\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584f5e19-7a54-4b6f-b7c8-2dcdbfd5958c",
   "metadata": {},
   "source": [
    "### Build HuggingFace Dataset from folder\n",
    "\n",
    "I should build an HF `Dataset` object and then use with a PyTorch `DataLoader` as in: https://huggingface.co/docs/datasets/en/use_with_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9082e715-4710-4529-929d-0b032b76ea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "label_map = {'happy': 1, 'sad': 0}\n",
    "\n",
    "# Download dataset from: https://github.com/mohummedalee/twitteraae-sentiment-data/\n",
    "def load_twitter_aae(dir):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    dialects = []\n",
    "    for dial in ['aae', 'sae']:\n",
    "        for lab in ['happy', 'sad']:\n",
    "            # load dialect x sentiment combination\n",
    "            fpath = os.path.join(dir, f'{dial}_{lab}')\n",
    "            with open(fpath, 'r', encoding='utf-8') as fh:\n",
    "                try:\n",
    "                    for line in fh:\n",
    "                        sentences.append(line.strip())\n",
    "                        labels.append(label_map[lab])\n",
    "                        dialects.append(dial.upper())\n",
    "                except UnicodeDecodeError:\n",
    "                    pass\n",
    "\n",
    "    return sentences, labels, dialects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6285b15e-525c-49f2-bb3f-576885d89202",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data/raw/sentiment_race'\n",
    "sentences, labels, dialects = load_twitter_aae(DATA_DIR)\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    'text': sentences,\n",
    "    'label': labels,\n",
    "    'dialect': dialects\n",
    "}).with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e28cdfae-6c84-4889-84a1-b63f4a5b3676",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f532be4a-fb4e-4d79-a910-7228f4179837",
   "metadata": {},
   "source": [
    "### Set up model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4bfde5d-737f-4ce1-9f75-8402a3f1cb80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0ee585bdbb42bfb81c72fea39fee01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"FacebookAI/roberta-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63d0f3b0-b872-49dd-82c4-e7d4581c5773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d0db2457ded4588aabd66ca4ce959c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=3):   0%|          | 0/12473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/muh.ali/miniconda3/envs/fairness-privacy/lib/python3.11/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/muh.ali/miniconda3/envs/fairness-privacy/lib/python3.11/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/muh.ali/miniconda3/envs/fairness-privacy/lib/python3.11/site-packages/datasets/table.py:1373: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n"
     ]
    }
   ],
   "source": [
    "# run tokenizer on dataset using datasets .map function\n",
    "MAXLEN = 128\n",
    "\n",
    "def tokenize(instance):\n",
    "    tokenized = tokenizer(instance['text'], truncation=True, padding=\"max_length\", max_length=MAXLEN)\n",
    "    # return {**tokenized, \"label\": instance['label'], \"dialect\": instance['dialect']}\n",
    "    return {**tokenized}\n",
    "    \n",
    "dataset = dataset.map(tokenize, num_proc=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9a75689-4f60-4a42-b9fc-23f7a919acec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'dialect', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 12473\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset\n",
    "\n",
    "# dataset now has `input_ids` and `attention_mask` fields too -- this is what is needed for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0abcf93-a24a-4384-94e7-38c4066b78ec",
   "metadata": {},
   "source": [
    "**TODOs**\n",
    "- split into train and validation set, build pytorch dataloaders for both\n",
    "- run raw training loop on the train_dataloader (as in https://huggingface.co/docs/transformers/en/training#train-in-native-pytorch) --- note that we don't want to use the `Trainer` API and should learn to write our own training loops since we will need to do that anyway when using `private-transformers` later (https://github.com/lxuechen/private-transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2315efe0-2f7a-4628-9b3f-db8a57b4bd5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
